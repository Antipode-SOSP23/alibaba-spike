#!/usr/bin/env python3

from pathlib import Path
from pprint import pprint as pp
from plumbum import local, path, FG
import os
import sys
import argparse
import time
import inspect
from datetime import date, datetime, time, timedelta
from time import sleep as tsleep

#--------------
# HELPERS
#--------------
def _load_yaml(path):
  import ruamel.yaml
  from jinja2 import Template

  with open(path, 'r') as f:
    yaml = ruamel.yaml.YAML()
    yaml.preserve_quotes = True
    data = yaml.load(f) or {}

    # hack to evaluate jinja2 variables
    for k, v in data.items():
      if isinstance(v, str) and '{{' in v:
        t = Template(v)
        data[k] = t.render(**data)

    return data

def _dump_yaml(path, d):
  from ruamel.yaml import YAML
  yaml=YAML()
  path.parent.mkdir(exist_ok=True, parents=True)
  with open(path, 'w+') as f:
    yaml.default_flow_style = False
    yaml.dump(d, f)

def _put_last(deploy_type,k,v):
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_DEPLOY_FILE[deploy_type]).exists():
    doc = _load_yaml(LAST_DEPLOY_FILE[deploy_type])
  # write new value and save to file
  doc[k] = v
  # create path dirs if needed
  _dump_yaml(LAST_DEPLOY_FILE[deploy_type],doc)

def _get_last(deploy_type,k):
  # if file does not exist create empty dict to write to
  if Path(LAST_DEPLOY_FILE[deploy_type]).exists():
    doc = _load_yaml(LAST_DEPLOY_FILE[deploy_type])
  else:
    # create file with default entries
    doc = {
    }
    _dump_yaml(LAST_DEPLOY_FILE[deploy_type],doc)

  # default last files entries
  return doc.get(k)

def _wait_url_up(url):
  import urllib.request
  while True:
    try:
      return_code = urllib.request.urlopen(url).getcode()
      if return_code == 200:
        return
    except (urllib.error.URLError, ConnectionResetError) as e:
      pass

# Services available:
#   hdfs_namenode
#   spark_master
#   jupyter_service
def _service_ip(deploy_type, service):
  inventory = _load_yaml(_get_last(args['deploy_type'],'inventory'))
  return inventory['nodes'][inventory[service]]['host']

def _copy_file_to_remote(remote, fro_path, to_path):
  from plumbum import local, path

  # make sure remote folder exist
  r_mkdir = remote['mkdir']
  r_mkdir['-p', str(to_path.parent)]()
  # copy files
  fro = local.path(fro_path)
  to = remote.path(to_path)
  path.utils.copy(fro, to)

def _flatten(l):
  out = []
  for sublist in l:
    out.extend(sublist)
  return out

#--------------
# CONSTANTS
#--------------
PROJECT_NAME = 'alibaba-spike'
SCRIPT_NAME = __file__.rsplit("/", 1)[1].split('.')[0]
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
DEPLOY_TYPES = [ 'gsd' ]
SPARK_DEPLOY_REPO_PATH = ROOT_PATH / 'spark-hdfs-deploy'
DEPLOY_PATH = ROOT_PATH / 'deploy'
SCRIPTS_PATH = ROOT_PATH / 'scripts'
STATS_PATH = ROOT_PATH / 'stats'
PLOTS_PATH = ROOT_PATH / 'plots'
HADOOP_DATASET_PATH = Path('hdfs:///dataset')
LAST_DEPLOY_FILE = { dp : DEPLOY_PATH / dp / f".last.yml" for dp in DEPLOY_TYPES }
YCSB_PROTOCOLS = [ 'Eventual', 'COPS', 'GentleRain' ]

#--------------
# DEPLOY
#--------------
def deploy(args):
  if not args['tag']:
    args['tag'] = f"{datetime.now().strftime('%Y%m%d%H%M')}"

  # make deploy path
  # save build variables for other commands
  _put_last(args['deploy_type'], 'inventory', args['inventory'])
  _put_last(args['deploy_type'], 'vars', args['vars'])
  _put_last(args['deploy_type'], 'tag', args['tag'])

  args['deploy_dir'] = DEPLOY_PATH / args['deploy_type'] / args['tag']

  getattr(sys.modules[__name__], f"deploy__{args['deploy_type']}")(args)
  print("[INFO] Deploy done!")

def deploy__gsd(args):
  import shutil
  from jinja2 import Environment
  import textwrap
  from plumbum.cmd import ansible_playbook

  deploy_dir = args['deploy_dir']
  inventory = _load_yaml(args['inventory'])
  deploy_vars = _load_yaml(args['vars'])

  print(f"[INFO] Copying deploy files... ", flush=True)
  os.makedirs(deploy_dir, exist_ok=True)
  shutil.copytree(SPARK_DEPLOY_REPO_PATH, deploy_dir, dirs_exist_ok=True)
  shutil.copy(args['vars'], deploy_dir / 'vars.yaml')

  print(f"[INFO] Generating inventory... ", flush=True)
  template = """
    [nodes]
    {% for hostname,node in nodes.items() %}{{ hostname }} ansible_host={{ node['host'] }} ansible_user="{{ user }}" ansible_ssh_private_key_file="{{ ssh_private_key_file }}"
    {% endfor %}

    [hdfs_namenode]
    {{ hdfs_namenode }}

    [hdfs_datanodes]
    {% for node in hdfs_datanodes %}{{ node }}
    {% endfor %}

    [spark_master]
    {{ spark_master }}

    [spark_workers]
    {% for node in spark_workers %}{{ node }}
    {% endfor %}

    [jupyter_service]
    {{ jupyter_service }}
  """
  template_render = Environment().from_string(template).render({
    'ssh_user': deploy_vars['default_user'],
    'ssh_private_key_file': deploy_vars['default_private_key_file'],
    #
    'nodes': inventory['nodes'],
    'hdfs_namenode': inventory['hdfs_namenode'],
    'hdfs_datanodes': inventory['hdfs_datanodes'],
    'spark_master': inventory['spark_master'],
    'spark_workers': inventory['spark_workers'],
    'jupyter_service': inventory['jupyter_service'],
  })
  inventory_filepath = deploy_dir / 'inventory.ini'
  with open(inventory_filepath, 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_render))
  print(f"[SAVED] '{inventory_filepath}'")

  if args['provision']:
    print(f"[INFO] Provision Spark + Hadoop stack... ", flush=True)
    with local.cwd(deploy_dir):
      ansible_playbook['stack-provision.yml'] & FG

  print(f"[INFO] Start Spark + Hadoop stack... ", flush=True)
  with local.cwd(deploy_dir):
    ansible_playbook['stack-start.yml'] & FG

  _wait_url_up(f"http://{_service_ip(args['deploy_type'],'spark_master')}:8080")
  _wait_url_up(f"http://{_service_ip(args['deploy_type'],'hdfs_namenode')}:9870")


#--------------
# INFO
#--------------
def info(args):
  args['tag'] = _get_last(args['deploy_type'], 'tag')
  args['inventory'] = _load_yaml(_get_last(args['deploy_type'],'inventory'))
  args['vars'] = _load_yaml(_get_last(args['deploy_type'],'vars'))
  args['deploy_dir'] = DEPLOY_PATH / args['deploy_type'] / args['tag']

  getattr(sys.modules[__name__], f"info__{args['deploy_type']}")(args)

def info__gsd(args):
  spark_master = _service_ip(args['deploy_type'], 'spark_master')
  hdfs_namenode = _service_ip(args['deploy_type'], 'spark_master')
  jupyter_service = _service_ip(args['deploy_type'], 'spark_master')

  print(f"\tSpark: http://{spark_master}:8080")
  print(f"\tSpark Resource Manager: http://{spark_master}:8088/cluster")
  print(f"\tHadoop Cluster Overview: http://{hdfs_namenode}:9870/dfshealth.html#tab-overview")
  print(f"\tHDFS Folder: http://{hdfs_namenode}:9870/explorer.html#/dataset")
  print(f"\tJupyter: http://{jupyter_service}:7777")


#--------------
# STATS
#--------------
def stats(args):
  args['tag'] = _get_last(args['deploy_type'], 'tag')
  args['deploy_dir'] = DEPLOY_PATH / args['deploy_type'] / args['tag']
  args['inventory'] = _load_yaml(_get_last(args['deploy_type'],'inventory'))
  args['vars'] = _load_yaml(_get_last(args['deploy_type'],'vars'))
  args['app_name'] = f"{args['tag']}-stats{'--sample' if args['sample'] else ''}"

  getattr(sys.modules[__name__], f"stats__{args['deploy_type']}")(args)
  print("[INFO] Stats done!")

def stats__gsd(args):
  from plumbum import SshMachine
  from plumbum.cmd import grep, cut

  print("[INFO] Connecting to master...")
  spark_master_host = _service_ip(args['deploy_type'], 'spark_master')
  remote = SshMachine(spark_master_host,
    ssh_opts=['-o ServerAliveInterval=9999'],
    user=args['vars']['default_user'],
    keyfile=args['vars']['default_private_key_file'])

  print("[INFO] Copying script file...")
  fro_path = SCRIPTS_PATH / args['script'].name
  to_path = Path(args['vars']['default_install_dir']) / 'scripts' / args['script'].name
  _copy_file_to_remote(remote, fro_path, to_path)

  print("[INFO] Submitting script file to Spark...")
  spark_master = f"spark://{spark_master_host}:7077"

  dataset = HADOOP_DATASET_PATH
  if args['sample']:
    # use first csv as sample
    dataset = dataset / 'MSCallGraph_0.csv'

  with remote.env(PATH=remote.expand('$HOME/.asdf/bin:$HOME/.asdf/shims:$PATH')):
    r_spark_submit = remote[str(Path(args['vars']['spark_home_dir']) / 'bin' / 'spark-submit')]

    r_spark_submit[
      '--master', spark_master,
      '-c', 'spark.ui.showConsoleProgress=true',
      # '-c', 'spark.sql.shuffle.partitions=100',
      '--executor-memory', '100g',
      str(to_path),
      spark_master,
      args['app_name'],
      dataset,
    ] & FG

  # merge all files into one
  print(f"[INFO] Finished spark submit!")

  # copy remote stats file to local
  print(f"[INFO] Copy stats file...")
  fro_path = Path(f"/tmp/{args['app_name']}.yml")
  to_path = STATS_PATH / fro_path.name
  path.utils.copy(remote.path(fro_path), local.path(to_path))


#--------------
# CLEAN
#--------------
def clean(args):
  args['tag'] = _get_last(args['deploy_type'], 'tag')
  args['deploy_dir'] = DEPLOY_PATH / args['deploy_type'] / args['tag']

  getattr(sys.modules[__name__], f"clean__{args['deploy_type']}")(args)
  print("[INFO] Clean done!")

def clean__gsd(args):
  from plumbum.cmd import ansible_playbook

  print(f"[INFO] Cleaning ... ", flush=True)
  with local.cwd(args['deploy_dir']):
    ansible_playbook['playbook-clean-caches.yml'] & FG
    if args['rebalance']:
      print(f"[INFO] Rebalancing HDFS... ", flush=True)
      ansible_playbook['playbook-hdfs-rebalance.yml'] & FG
    if args['stop']:
      print(f"[INFO] Stopping stack... ", flush=True)
      ansible_playbook['stack-stop.yml'] & FG
    if args['strong']:
      print(f"[INFO] Decommissioning stack... ", flush=True)
      ansible_playbook['stack-decomission.yml'] & FG


#--------------
# PLOTS
#--------------
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LogNorm
import matplotlib.ticker as mtick
from matplotlib.dates import MinuteLocator, DateFormatter
import matplotlib.gridspec as gridspec
BASE_FIGSIZE = [6,4]
BASE_DPI = 600
BASE_COLOR_PALETTE = sns.color_palette('deep', 12)
COLORS = {
  'baseline': BASE_COLOR_PALETTE[0],
  'oc':       BASE_COLOR_PALETTE[1],
  'sl': BASE_COLOR_PALETTE[0],
  'sf': BASE_COLOR_PALETTE[1],
}
LABELS = {
  'sl': 'Stateless',
  'sf': 'Stateful',
}
LTX_SYMBOLS = {
  # https://matplotlib.org/3.5.0/tutorials/text/mathtext.html
  'approx' : r'$\approx$',
}
PERCENTILES=[0.20, 0.25, 0.30, 0.40, 0.5, 0.60, 0.75, 0.80, 0.90, 0.95, 0.99]

def _save_plot_file(tight=True, tag=None):
  if tight: plt.tight_layout()

  parts = [
    inspect.stack()[1][3].split('__')[1], # plot name
    tag,
    datetime.now().strftime('%Y%m%d%H%M') # datetime
  ]
  plot_filename = '__'.join([x for x in parts if x is not None]) + '.svg'
  plt.savefig(PLOTS_PATH / plot_filename, bbox_inches = 'tight', pad_inches = 0.1, format='svg', dpi=600)

  print(f"[INFO] Saved plot '{plot_filename}'")
  plt.clf()
  plt.close('all')

#--------------

def plot(args):
  args['stats_data'] = [ _load_yaml(Path(d)) for d in args['stats'] ]

  for plot_name in args['plots']:
    getattr(sys.modules[__name__], f"plot__{plot_name}")(args)
  print("[INFO] Plot done!")

def plot__trace_timeline(args):
  # this plot only accepts one instance of data
  dataset_name = args['stats'][0]
  dataset = args['stats_data'][0]
  df = pd.DataFrame.from_dict(dataset['timeline_per_sec'], orient='index')\
    .reset_index()\
    .rename(columns={'index': 'ts', 0: 'num_reqs'})

  # rescale original dataset to datetime starting at midnight
  midnight = datetime.combine(date.today(), time.min)
  df['ts'] = df['ts'].map(lambda secs: midnight + timedelta(seconds=secs))

  # Apply theme
  sns.set_theme(style='ticks')
  plt.rcParams['figure.dpi'] = BASE_DPI
  plt.rcParams['figure.figsize'] = [6,6]
  plt.rcParams['axes.labelsize'] = 'small'

  f = sns.lineplot(data=df, x='ts', y='num_reqs')

  # set axis labels
  f.set_xlabel('')
  f.set_ylabel('req/s')

  # remove the day from the xtick and keep only time
  new_xticks_labels = [ l.get_text().split(' ',1)[1] for l in f.get_xticklabels() ]
  f.set_xticklabels(labels=new_xticks_labels, rotation=45)

  _save_plot_file()

def plot__cdf_meta_rcptype_unique_services_and_calls(args):
  # this plot only accepts one instance of data
  dataset = args['stats_data'][0]

  unique_sf_hist_df = pd.DataFrame.from_dict([ {'count': v, 'meta_rpctype': 'sf'}  for v in _flatten([ e['count'] * [ e['count_bin'] ] for e in dataset['unique_sf_per_trace_hist'] ]) ], orient='columns')
  unique_sl_hist_df = pd.DataFrame.from_dict([ {'count': v, 'meta_rpctype': 'sl'}  for v in _flatten([ e['count'] * [ e['count_bin'] ] for e in dataset['unique_sl_per_trace_hist'] ]) ], orient='columns')

  calls_unique_by_metarpctype_df = pd.concat([unique_sf_hist_df, unique_sl_hist_df], ignore_index=True)[['count','meta_rpctype']]\
    .sort_values(by=['meta_rpctype'], ascending=[True])\
    .reset_index(drop=True)

  sf_hist_df = pd.DataFrame.from_dict([ {'count': v, 'meta_rpctype': 'sf'}  for v in _flatten([ e['count'] * [ e['count_bin'] ] for e in dataset['sf_per_trace_hist'] ]) ], orient='columns')
  sl_hist_df = pd.DataFrame.from_dict([ {'count': v, 'meta_rpctype': 'sl'}  for v in _flatten([ e['count'] * [ e['count_bin'] ] for e in dataset['sl_per_trace_hist'] ]) ], orient='columns')

  calls_by_metarpctype_df = pd.concat([sf_hist_df, sl_hist_df], ignore_index=True)[['count','meta_rpctype']]\
    .sort_values(by=['meta_rpctype'], ascending=[True])\
    .reset_index(drop=True)

  # Apply the default theme
  print(f"[INFO] Plotting")
  sns.set_theme(style='ticks')
  plt.rcParams['figure.dpi'] = BASE_DPI
  plt.rcParams['figure.figsize'] = [6,1]
  plt.rcParams['axes.labelsize'] = 'xx-small'
  plt.rcParams['xtick.labelsize'] = 'xx-small'
  plt.rcParams['ytick.labelsize'] = 'xx-small'
  plt.rcParams['legend.fontsize'] = 'xx-small'

  fig,ax = plt.subplots(1,2, gridspec_kw={'wspace':0.1}, sharey=True)

  plot_colors = { k: COLORS[k] for k in COLORS if k in ['sf','sl'] }
  f0 = sns.ecdfplot(ax=ax[0], data=calls_by_metarpctype_df, x='count', hue='meta_rpctype', log_scale=False, palette=plot_colors, linewidth=1.5)
  f1 = sns.ecdfplot(ax=ax[1], data=calls_unique_by_metarpctype_df, x='count', hue='meta_rpctype', log_scale=False, palette=plot_colors, linewidth=1.5)

  # limits
  pp(calls_by_metarpctype_df.quantile(PERCENTILES))
  pp(calls_unique_by_metarpctype_df.quantile(PERCENTILES))
  f0_cut_percentile = calls_by_metarpctype_df.quantile(0.95)['count']
  f1_cut_percentile = calls_unique_by_metarpctype_df.quantile(0.99)['count']

  f0.set_xlim(left=-0.1, right=f0_cut_percentile)
  f1.set_xlim(left=-0.1, right=f1_cut_percentile)

  # fix number of ticks
  ax[0].yaxis.set_major_locator(mtick.MultipleLocator(base=0.25))
  ax[0].xaxis.set_major_locator(mtick.MultipleLocator(base=20.0))
  ax[1].xaxis.set_major_locator(mtick.MultipleLocator(base=10.0))

  # remove 0 labels
  ax[0].xaxis.get_major_ticks()[1].label1.set_visible(False)
  ax[1].xaxis.get_major_ticks()[1].label1.set_visible(False)
  ax[0].yaxis.get_major_ticks()[1].label1.set_visible(False)

  # set title as label with percentile
  f0.set_title(f"p95th",loc='left',fontdict={'fontsize': 'xx-small'}, style='italic')
  f1.set_title(f"p99th",loc='left',fontdict={'fontsize': 'xx-small'}, style='italic')

  # set axis labels
  f0.set_xlabel('# of Calls per Request')
  f1.set_xlabel('# of Unique Services per Request')
  f0.set_ylabel('')
  f1.set_ylabel('')

  # remove legend from left plot
  f0.legend_.remove()

  # replace legend labels
  for legend_label in f1.legend_.get_texts():
    legend_label.set_text(LABELS[legend_label.get_text()].replace('\n', ' '))

  # remove legend title
  f1.legend_.set_title(None)

  # move legend to correct spot
  sns.move_legend(f1, "lower right", bbox_to_anchor=(1.015, 1.0), ncol=2)

  _save_plot_file(tight=False)

#--------------
# CLI
#--------------
PLOTS_AVAILABLE = [m.split('plot__')[1] for m,_ in inspect.getmembers(sys.modules[__name__]) if m.startswith('plot__')]
SHRINK_STRATEGIES_AVAILABLE = [m.split('__')[2] for m,_ in inspect.getmembers(sys.modules[__name__]) if m.startswith('shrink__')]
if __name__ == '__main__':
  # parse arguments
  main_parser = argparse.ArgumentParser()

  # deploy flag
  deploy_type_group = main_parser.add_mutually_exclusive_group(required=True)
  for dt in DEPLOY_TYPES:
    deploy_type_group.add_argument(f"--{dt}", action='store_true')

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # deploy
  deploy_parser = subparsers.add_parser('deploy', help='Deploy application')
  deploy_parser.add_argument('-inventory', required=True, help="Inventory configuration")
  deploy_parser.add_argument('-vars', required=True, help="Deploy vars")
  deploy_parser.add_argument('-provision', action='store_true', help="By default we do not provision the stack")
  deploy_parser.add_argument('-tag', required=False, help="Deploy with already existing tag")

  # info
  info_parser = subparsers.add_parser('info', help='Application info')

  # stats
  stats_parser = subparsers.add_parser('stats', help='Gather stats over dataset')
  stats_parser.add_argument('-script', type=Path, required=False, default=SCRIPTS_PATH / 'orig_stats.py')
  stats_parser.add_argument('-sample', action='store_true')

  # clean
  clean_parser = subparsers.add_parser('clean', help='Clean application')
  clean_parser.add_argument('-rebalance', action='store_true', help="Rebalance dfs")
  clean_parser.add_argument('-stop', action='store_true', help="Stop stack")
  clean_parser.add_argument('-strong', action='store_true', help="Decomission stack")

  # plot stats
  plot_parser = subparsers.add_parser('plot', help='Plot stats')
  plot_parser.add_argument('-stats', nargs='+', type=str, required=True)
  plot_parser.add_argument('-plots', nargs='*', choices=PLOTS_AVAILABLE, default=PLOTS_AVAILABLE, required=False, help="Plot only the passed plot names")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which').replace('-', '_')

  # parse deploy type
  args['deploy_type'] = None
  for dt in DEPLOY_TYPES:
    if args[dt]:
      args['deploy_type'] = dt
    del args[dt]

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)